{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of metrics\n",
    "\n",
    "In this notebook, we compare the readability scores computed with our metrics with those computed using the python library `textstat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textstat in /opt/conda/lib/python3.7/site-packages (0.6.2)\r\n",
      "Requirement already satisfied: pyphen in /opt/conda/lib/python3.7/site-packages (from textstat) (0.9.5)\r\n"
     ]
    }
   ],
   "source": [
    "# Run when first starting up notebook\n",
    "!pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import textstat\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "max_len = 670\n",
    "raw_data_file = \"raw_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data\n",
    "def load_raw_data(in_file):\n",
    "    with open(in_file) as f:\n",
    "        data = json.load(f)\n",
    "        df = pd.json_normalize(data)\n",
    "\n",
    "    df['from.text'] = df['from.text'].astype('str')\n",
    "    df['to.text'] = df['to.text'].astype('str')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean data\n",
    "def clean_data(df: pd.DataFrame, max_len: int, dirty_text: List[str]) -> pd.DataFrame:\n",
    "    is_from_text_dirty = df['from.text'].str.contains('|'.join(dirty_text))\n",
    "    is_to_text_dirty = df['to.text'].str.contains('|'.join(dirty_text))\n",
    "    is_text_ok_length = (df['from.text'].map(len) < max_len) & (df['to.text'].map(len) < max_len)\n",
    "    return df[~is_from_text_dirty & ~is_to_text_dirty & is_text_ok_length].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean df\n",
    "def load_clean_df():\n",
    "    dirty_text = ['reference-type=\"ref\"', 'style=\"color', '#tab:', 'smallcaps', '\\$']\n",
    "    df = load_raw_data(raw_data_file)\n",
    "    return clean_data(df, max_len, dirty_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_all_paragraphs(_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_from = pd.DataFrame()\n",
    "    df_from['Text'] = _df['from.text']\n",
    "    df_from['FRE'] = _df['from.readability.fleschReadingEase']\n",
    "    df_from['FKG'] = _df['from.readability.fleschKincaidGradeLevel']\n",
    "    df_from['DCS'] = _df['from.readability.daleChallScore']\n",
    "    df_from['PDS'] = _df['from.readability.papersDomainScore']\n",
    "    df_to = pd.DataFrame()\n",
    "    df_to['Text'] = _df['to.text']\n",
    "    df_to['FRE'] = _df['to.readability.fleschReadingEase']\n",
    "    df_to['FKG'] = _df['to.readability.fleschKincaidGradeLevel']\n",
    "    df_to['DCS'] = _df['to.readability.daleChallScore']\n",
    "    df_to['PDS'] = _df['to.readability.papersDomainScore']\n",
    "    return pd.concat([df_from, df_to])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = to_all_paragraphs(load_clean_df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add texstat metrics and deltas\n",
    "def flesch_reading_ease(col_name: str):\n",
    "    return lambda row: textstat.flesch_reading_ease(row[col_name])\n",
    "\n",
    "def flesch_kincaid_grade_level(col_name: str):\n",
    "    return lambda row: textstat.flesch_kincaid_grade(row[col_name])\n",
    "\n",
    "def dale_chall_score(col_name: str):\n",
    "    return lambda row: textstat.dale_chall_readability_score(row[col_name])\n",
    "\n",
    "df['textstat.FRE'] = df.apply(flesch_reading_ease('Text'), axis = 1)\n",
    "df['textstat.FKG'] = df.apply(flesch_kincaid_grade_level('Text'), axis = 1)\n",
    "df['textstat.DCS'] = df.apply(dale_chall_score('Text'), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   FRE  textstat.FRE\n",
      "FRE           1.000000      0.879123\n",
      "textstat.FRE  0.879123      1.000000\n",
      "\n",
      "                   FKG  textstat.FKG\n",
      "FKG           1.000000      0.814065\n",
      "textstat.FKG  0.814065      1.000000\n",
      "\n",
      "                   DCS  textstat.DCS\n",
      "DCS           1.000000      0.795208\n",
      "textstat.DCS  0.795208      1.000000\n",
      "\n",
      "          PDS       DCS       FRE       FKG\n",
      "PDS  1.000000  0.707330 -0.597507  0.595911\n",
      "DCS  0.707330  1.000000 -0.723489  0.653777\n",
      "FRE -0.597507 -0.723489  1.000000 -0.892796\n",
      "FKG  0.595911  0.653777 -0.892796  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Check correlation of our metrics vs textstat\n",
    "def correlate_metrics(_df: pd.DataFrame):\n",
    "    metrics_fre = _df[[\n",
    "        'FRE',\n",
    "        'textstat.FRE',\n",
    "    ]]\n",
    "    metrics_fkg = _df[[\n",
    "        'FKG',\n",
    "        'textstat.FKG',\n",
    "    ]]\n",
    "    metrics_dcs = _df[[\n",
    "        'DCS',\n",
    "        'textstat.DCS',\n",
    "    ]]\n",
    "    metrics_pds = _df[[\n",
    "        'PDS',\n",
    "        'DCS',\n",
    "        'FRE',\n",
    "        'FKG'\n",
    "    ]]\n",
    "\n",
    "    print(metrics_fre.corr().to_string())\n",
    "    print()\n",
    "    print(metrics_fkg.corr().to_string())\n",
    "    print()\n",
    "    print(metrics_dcs.corr().to_string())\n",
    "    print()\n",
    "    print(metrics_pds.corr().to_string())\n",
    "    \n",
    "correlate_metrics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate_metrics_graphical(_df: pd.DataFrame):\n",
    "    metrics_cols = ['FRE', 'FKG', 'DCS', 'PDS']\n",
    "    data = _df[metrics_cols]\n",
    "    sns.pairplot(data, height = 3, kind=\"reg\")\\\n",
    "        .fig.suptitle(\n",
    "            f\"Correlation of readability metrics on paragraphs\",\n",
    "            y=1.05,\n",
    "            size='x-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlate_metrics_graphical(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
