{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Survey Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import json\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "max_len = 670\n",
    "raw_data_file = \"raw_data.json\"\n",
    "md_file = \"raw_data.md\"\n",
    "clean_data_questions_file = \"data_clean_questions.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data\n",
    "def load_raw_data(in_file):\n",
    "    with open(in_file) as f:\n",
    "        data = json.load(f)\n",
    "        df = pd.json_normalize(data)\n",
    "\n",
    "    df['from.text'] = df['from.text'].astype('str')\n",
    "    df['to.text'] = df['to.text'].astype('str')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of MarkDown file with questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to render data as a Markdown file\n",
    "def generate_markdown(df: pd.DataFrame, fname: str):\n",
    "    def format_delta(d):\n",
    "        return f\"+{d}\" if d > 0 else d\n",
    "        \n",
    "    with open(fname, 'w+') as f:\n",
    "        f.write(\"# Dump of changes\\n\\n\")\n",
    "        for index, row in df.iterrows():\n",
    "            from_id, to_id = row[['from.commitId', 'to.commitId']]\n",
    "            from_author, to_author = row[['from.commitAuthorEmail', 'to.commitAuthorEmail']]\n",
    "            from_text, to_text = row[['from.text', 'to.text']]\n",
    "            f.write(f\"## Change in {row['documentRepoName']}\\n\")\n",
    "            f.write(f\"id: {row['_id']}  \\n\")\n",
    "            f.write(f\"Flesch reading ease: {format_delta(row['freDelta'])}  \\n\")\n",
    "            f.write(f\"Fleschâ€”Kincaid grade level: {format_delta(row['fkglDelta'])}  \\n\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(f\"| **Version {from_id}** | **Version {to_id}** |\\n\")\n",
    "            f.write(f\"| :------ | :------ |\\n\")\n",
    "            f.write(f\"| *{from_author}* | *{to_author}* |\\n\")\n",
    "            f.write(f\"| ------------------ | ------------------ |\\n\")\n",
    "            f.write(f\"| {from_text} | {to_text} |\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run generate md\n",
    "def run_gen_md():\n",
    "    df = load_raw_data(raw_data_file)\n",
    "    generate_markdown(df, md_file)\n",
    "\n",
    "run_gen_md()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of survey questions (qualtrics TXT format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean data\n",
    "def clean_data(df: pd.DataFrame, max_len: int, dirty_text: List[str]) -> pd.DataFrame:\n",
    "    is_from_text_dirty = df['from.text'].str.contains('|'.join(dirty_text))\n",
    "    is_to_text_dirty = df['to.text'].str.contains('|'.join(dirty_text))\n",
    "    is_text_ok_length = (df['from.text'].map(len) < max_len) & (df['to.text'].map(len) < max_len)\n",
    "    return df[~is_from_text_dirty & ~is_to_text_dirty & is_text_ok_length].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a single question\n",
    "def gen_question(_id: str, text_a: str, text_b: str, swap: bool) -> str:\n",
    "    id_str = f\"{_id}-rev\" if swap else _id\n",
    "    a, b = (text_b, text_a) if swap else (text_a, text_b)\n",
    "    return f\"\"\"\n",
    "[[Question:MC]]\n",
    "[[ID:{id_str}]]\n",
    "\n",
    "<div><strong><span style=\"font-size:16px;\">Paragraph A</span></strong></div>\n",
    "<div>{a}</div>\n",
    "<br/>\n",
    "<div><strong><span style=\"font-size:16px;\">Paragraph B</span></strong></div>\n",
    "<div>{b}</div>\n",
    "<br/><hr/><br/>\n",
    "<div>Paragraph <strong>A</strong> is more readable than paragraph <strong>B</strong>.</div>\n",
    "<br/>\n",
    "\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write questions ndjson\n",
    "def generate_questions(df: pd.DataFrame, fname: str, q_per_block: int):\n",
    "    if (q_per_block % 2 != 0):\n",
    "        raise Exception('q_per_block must be even')\n",
    "\n",
    "    survey_preamble = \"\"\"[[AdvancedFormat]]\n",
    "\n",
    "[[Block:Premble]]\n",
    "\n",
    "[[Question:Text]]\n",
    "<div>Hi There. Thank you for taking the time to take part in this study.</div>\n",
    "\n",
    "[[Question:MC]]\n",
    "[[ID:Qlevel]]\n",
    "First, a question about you. What is your education level?\n",
    "\n",
    "[[Choices]]\n",
    "Bachelor's degree\n",
    "Master's degree\n",
    "PhD\n",
    "Professor\n",
    "\n",
    "[[Block:Information]]\n",
    "\n",
    "[[Question:Text]]\n",
    "<div>This study is about text readability in the context of software engineering papers.</div>\n",
    "<div>You will be presented with pairs of paragraphs, and for each pair you will have to state how much you agree with a statement regarding their readability.</div>\n",
    "<br/>\n",
    "<div>The <strong>readability</strong> of a text indicates how easy it is to read.</div>\n",
    "<div>It is the <strong>ease of reading</strong> created by the choice of content, style, design, and organization that fit the prior knowledge, reading skill, interest, and motivation of the audience.</div>\n",
    "<br/>\n",
    "<div>Readability is <strong>not</strong> to be confused with legibility, which is more concerned with the visual perception and the layout of the text.</div>\n",
    "\"\"\"\n",
    "    survey_block_header = \"[[Block]]\\n\"\n",
    "    survey_question_choices = \"\"\"[[Choices]]\n",
    "Strongly agree\n",
    "Somewhat agree\n",
    "Neither agree nor disagree\n",
    "Somewhat disagree\n",
    "Strongly disagree\n",
    "\"\"\"\n",
    "    with open(fname, 'w+') as f:\n",
    "        f.write(survey_preamble)\n",
    "        f.write(\"\\n\")\n",
    "        for i, row in df.iterrows():\n",
    "            if (i % (q_per_block / 2) == 0):\n",
    "                f.write(\"\\n\")\n",
    "                f.write(survey_block_header)\n",
    "            # Write question\n",
    "            f.write(gen_question(row['_id'], row['from.text'], row['to.text'], False))\n",
    "            f.write(survey_question_choices)\n",
    "            # Write reversed question (guaranteed to be in the same block)\n",
    "            f.write(gen_question(row['_id'], row['from.text'], row['to.text'], True))\n",
    "            f.write(survey_question_choices)\n",
    "\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run generate clean data questions\n",
    "def run_gen_questions():\n",
    "    dirty_text = ['reference-type=\"ref\"', 'style=\"color', '#tab:', 'smallcaps', '\\$']\n",
    "    df = load_raw_data(raw_data_file)\n",
    "    clean_df = clean_data(df, max_len, dirty_text)\n",
    "    num_blocks = 5\n",
    "    questions_per_block = len(clean_df) / num_blocks\n",
    "    generate_questions(clean_df, clean_data_questions_file, questions_per_block)\n",
    "    \n",
    "run_gen_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
